{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "### Introduction to Machine Learning\n",
    "\n",
    "This assignment is to be done with a partner. Please only submit ONE .ipynb (not .py) file per pair!\n",
    "\n",
    "There are **11** problems in this assignment\n",
    "\n",
    "**Total points: 12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this laboratory we will experiment with text classification. The inputs to our machine learning models will be texts, each of which has a label (0 or 1). The model is trained to classify new texts as 0 or 1. In particular, we will use the dataset from the shared task *Presupposed Taxonomies: Evaluating Neural Network Semantics (PreTENS)*, part of the SemEval 2022 conference:\n",
    "\n",
    "https://sites.google.com/view/semeval2022-pretens\n",
    "\n",
    "The data consists of simple sentences, such as these:\n",
    "\n",
    "    * I like trees, and in particular birches\n",
    "    * I like oaks, and in particular trees\n",
    "    \n",
    "Some of the sentences (like the first one shown above) make sense, while others (like the second one shown above) do not. The reason that these sentences do not make sense is that the relationship between the two nouns included in the sentence is not compatible with the pattern where the two nouns are inserted.\n",
    "Sentences that make sense are labeled as `1` (*acceptable*), while sentences that do not make sense are labeled as `0` (*unacceptable*). The labeling is done by human annotators.\n",
    "\n",
    "Participants in this shared task were asked to train a machine learning model to predict whether sentences are acceptable or unacceptable. The best model was then applied to a secret test set that was not provided to the participants, and the highest score won.\n",
    "\n",
    "We have downloaded the data from this task from their [Github repository](https://github.com/shammur/SemEval2022Task3). In total, we have approximately 20000 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "In the last part of the lab, we will train neural networks. For this, you will need to install tensorflow if you haven't already. We recommend installing TensorFlow using Anaconda: from the Anaconda prompt, type\n",
    "\n",
    "pip install tensorflow\n",
    "\n",
    "That should be sufficient to install TensorFlow. If needed, platform-specific installation instructions can be found here:\n",
    "\n",
    "https://www.tensorflow.org/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following modules and packages are needed to load the data and to manipulate them. **Unless otherwise stated**, however, **DO NOT** use any method from these modules for any ML-related goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (15, 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "For this assignment we pre-processed the sentences by tokenizing the text and annotating the grammatical category and lemma of each token (more details below). These data are available in the `labeled_data_preprocessed.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labeled_data_preprocessed.csv\", index_col = 4)\n",
    "annotation_re = re.compile(r\"(.+)/(.+)\")\n",
    "df['pos_tagged_sentence'] = df['pos_tagged_sentence'].apply(lambda x: [annotation_re.findall(s)[0] for s in x.split()])\n",
    "df['lemmatized_sentence'] = df['lemmatized_sentence'].apply(lambda x: [annotation_re.findall(s)[0] for s in x.split()])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on Pandas\n",
    "\n",
    "By executing the code above we've loaded our csv file as a Python Pandas DataFrame. If you are not familiar with this data structure you can easily cast our DataFrame into a  **dictionary of dictionaries**, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = df.to_dict()\n",
    "\n",
    "# note that the keys of the outmost dictionary are the column names\n",
    "print(df_dict.keys())\n",
    "\n",
    "# while the keys of each embedded dictionary are the rows indices\n",
    "print(list(df_dict['raw_sentence'].keys())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns keys\n",
    "\n",
    "**`class`** = class label the sentence (**this is our target label**).\n",
    "\n",
    "    0 - unacceptable\n",
    "    1 - acceptable\n",
    "   \n",
    "**`raw_sentence`** = raw sentence text\n",
    "\n",
    "**`pos_tagged_sentence`** = an annotated version of each sentence obtained by:\n",
    "\n",
    "    - lowercasing the sentence\n",
    "    - tokenizing the text (roughly, splitting the into morphological units such as words and punctuation marks)\n",
    "    - annotating the grammatical category (a.k.a. Part of Speech: PoS) of each token\n",
    "\n",
    "**`lemmatized_sentence`** = annotated version of each sentence obtained by using the annotation available in the `pos_tagged_sentence` representation in order to tag the base form (e.g. \"*be*\" for the inflected word \"*am*\") of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging notation\n",
    "\n",
    "The PoS-tagged and the lemmatized sentences have been represented using the [nltk notation](http://www.nltk.org/book_1ed/ch05.html), where \n",
    "\n",
    "- each **sentence** is represented as a list of annotated tokens\n",
    "\n",
    "- each **annotated token** is represented as a `(word, tag)` tuple\n",
    "\n",
    "For instance, we can verbalize this PoS annotation:\n",
    "\n",
    "`[('The', 'DET'), ('grand', 'ADJ'), ('jury', 'NOUN'), ('commented', 'VERB')]`\n",
    "\n",
    "as:\n",
    "\n",
    "> *\"the determiner \"the\" is followed by the adjective \"grand\", in turn followed by the noun \"jury\" and the verb \"commented\"*\n",
    "\n",
    "... while the following lemmatized text \n",
    "\n",
    "`[('The', 'The'), ('grand', 'grand'), ('jury', 'jury'), ('commented', 'comment')]`\n",
    "\n",
    "can be read as \n",
    "\n",
    "> *the text is composed by inflected forms of the lemmas \"the\", \"grand\", \"jury\" and \"comment\"* (note that the only token that is different from its base form is the last one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech TagSet\n",
    "\n",
    "For PoS-tagging our sentences we've used the Universal tagset proposed by [Petrov et al (2011)](https://arxiv.org/abs/1104.2086):\n",
    "\n",
    "|  tag  | meaning |\n",
    "|:-----:|:---------|\n",
    "| VERB | verbs (all tenses and modes) |\n",
    "| NOUN | nouns (common and proper) |\n",
    "| PRON | pronouns |\n",
    "| ADJ | adjectives |\n",
    "| ADV | adverbs |\n",
    "| ADP | adpositions (prepositions and postpositions) |\n",
    "| CONJ | conjunctions |\n",
    "| DET | determiners (e.g., articles) |\n",
    "| NUM | cardinal numbers |\n",
    "| PRT | particles or other function words |\n",
    "| X | other: foreign words, typos, abbreviations |\n",
    "| \\. | punctuation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of the judgments\n",
    "\n",
    "As shown by the following barplot, about half of the sentences belong to each class. In other words, we are going to work with a **balanced dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts(sort = False).plot.bar(color='#1f77b4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Feature extraction\n",
    "\n",
    "In this step, we'll extract the features that we will later used to convert each sentence into a vector. Let's store all the estimates as subdictionaries of a `features_dictionary` dictionary of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys are feature names, values are dictionaries mapping sentence indices to feature values\n",
    "features_dictionary = dict() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** **Number of tokens in each sentence** (0.5 points)\n",
    "\n",
    "Calculate the number of tokens (i.e. linguistic units) in each sentence. \n",
    "\n",
    "Store these numbers in a `features_dictionary` sub-dictionary labelled `tokens_counts`, whose keys are the `sentence_id` and the values are the number of tokens of each sentence. \n",
    "\n",
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_token_counts():\n",
    "    key = 'tokens_counts'\n",
    "    assert len(features_dictionary) == 1, 'features_dictionary should contain a single key \"tokens_counts\"'\n",
    "    assert len(features_dictionary[key]) == len(df), f\"{len(features_dictionary[key])} {key}, {len(df)} sentences\"\n",
    "    assert all([el >= 0 for el in features_dictionary[key].values()]), f'{key} should be non-negative'\n",
    "    assert all([type(el) == int for el in features_dictionary[key].values()]), f'{key} should be integers'\n",
    "    assert set(features_dictionary[key].keys()) == set(df.index.tolist()), f'{key} indices should match sentence ids'\n",
    "\n",
    "test_token_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** **Number of characters in each sentence** (0.5 points)\n",
    "\n",
    "Calculate the number of characters in each **lemmatized sentence**. \n",
    "\n",
    "Store this estimates in a `features_dictionary` sub-dictionary labelled `characters_counts`, whose keys are the `sentence_id` and the values are the number of characters of each sentence. \n",
    "\n",
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_char_counts():\n",
    "    key = 'characters_counts'\n",
    "    assert len(features_dictionary) == 2, 'features_dictionary keys should be \"tokens_counts\" and \"characters_counts\"'\n",
    "    assert len(features_dictionary[key]) == len(df), f\"{len(features_dictionary[key])} {key}, {len(df)} sentences\"\n",
    "    assert all([el >= 0 for el in features_dictionary[key].values()]), f'{key} should be non-negative'\n",
    "    assert all([type(el) == int for el in features_dictionary[key].values()]), f'{key} should be integers'\n",
    "    assert set(features_dictionary[key].keys()) == set(df.index.tolist()), f'{key} indices should match sentence ids'\n",
    "\n",
    "test_char_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** **n-grams features** (1 point)\n",
    "\n",
    "As you have learned (or will shortly learn) in *Computationele Linguistiek*, a common way to enhance features is to use $n$-grams, i.e., a contiguous sequence of $n$ words. Write a piece of code that:\n",
    "\n",
    "a) finds the 200 most frequent $n$-grams in our *lemmatized* sentences, with $n$ equal to 1, 2, or 3. Store these $n$-grams in a set called `most_frequent_ngrams`\n",
    "\n",
    "b) counts how many times each of these `most_frequent_ngrams` appears in each sentence and stores these estimates in a dictionary of dictionaries called `ngrams_frequencies`, in which the keys are the $n$-grams (e.g. `my god`) and the values are dictionaries of the form `{sentence_id : occurrences_of_this_ngram}`. \n",
    "\n",
    "*Hint*: the `nltk.ngrams(sequence, n)` method can be used to generate all the possible length-$n$ $n$-grams from a sequence, as in the following examples (ignore eventual `DeprecationWarning` warnings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(nltk.ngrams([\"I\", \"came\", \"here\", \"to\", \"rescue\", \"you\", \"from\", \"him\"], 2)))\n",
    "print(list(nltk.ngrams([\"I\", \"came\", \"here\", \"to\", \"rescue\", \"you\", \"from\", \"him\"], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ngrams_features():\n",
    "    assert len([k for k,v in ngrams_frequencies['i like'].items() if v > 0]) == 13677.\n",
    "    assert ngrams_frequencies['his'][45] == 2\n",
    "    assert len(ngrams_frequencies) == 200\n",
    "    \n",
    "test_ngrams_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorganizing our features\n",
    "\n",
    "Up to this moment, we have organized our features into the following dictionaries:\n",
    "\n",
    "1. `ngrams_frequencies`: a group of lexical features, encoding how many times a given sequence of words (i.e. an n-gram) occurs in each sentence\n",
    "\n",
    "1. `features_dictionary`: the numbers of tokens and characters in each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our models, we're going to convert these dictionaries into matrices, where each row vector represent a single sentence. \n",
    "\n",
    "The following script converts the `features_dictionary` into a `other_features_matrix`, whose columns are the keys of `features_dictionary`, and the rows are the sentence ids. Similarly, it converts `ngrams_frequencies` into `ngrams_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(feature_dict: dict, sentence_ids: list) -> np.ndarray:\n",
    "    # feature_dict is one of features_dictionary, ngrams_frequencies\n",
    "    feature_names = feature_dict.keys()\n",
    "    # let's create a matrix of zeros and populate it by iterating over our dictionary \n",
    "    feature_matrix = np.zeros((len(sentence_ids), len(feature_names)))\n",
    "    for fix, feature_name in enumerate(feature_names):\n",
    "        for tix, sentence_id in enumerate(sentence_ids):\n",
    "            feature_matrix[tix, fix] = feature_dict[feature_name][sentence_id]\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ids = sorted(df_dict['raw_sentence'].keys())\n",
    "\n",
    "other_features_matrix = to_matrix(features_dictionary, sentence_ids)\n",
    "ngrams_matrix = to_matrix(ngrams_frequencies, sentence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable\n",
    "\n",
    "Next, we create an array with the class label for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_classes = np.asarray([df_dict['class'][i] for i in sentence_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting the lexical feature frequencies\n",
    "\n",
    "Absolute frequencies may be a deceiving source of information: a word may appear frequently in a sentence simply due to its overall high frequency, while the use of an overall rare word may be an useful semantic or pragmatic clue. As a consequence, it is common practice to **weight** the frequency of a word so as to take into account its overall distribution (sometimes, along with the distribution of all the words in the corpus). \n",
    "\n",
    "Probably the most widely used statistic in information retrieval and text mining is the [term frequency-inverse document frequency (tf-idf)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), in which the weigth of each term $t$ in each document $d$ is estimated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tf-idf}_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t = \\text{tf}_{t,d} \\times log \\left( \\frac{N}{\\text{df}_t}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- where:\n",
    "    - tf$_{t,d}$ is the number of occurrences of term $t$ in document $d$ (the term frequency)\n",
    "    - $N$ is the total number of documents\n",
    "    - df$_t$ is the number of documents in which term $t$ appears (the document frequency)\n",
    "    \n",
    "We transform `ngrams_matrix` into tf-idf representations called `ngrams_matrix_tfidf` using the `TfidfTransformer()` from sklearn (see the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) for more info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's weight our matrix (norm=\"l1\" because we chose to adjust the term frequency for document length)\n",
    "ngrams_matrix_tfidf = TfidfTransformer(norm=\"l1\").fit_transform(ngrams_matrix).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging our feature matrices\n",
    "\n",
    "**4. Merging our feature matrices** (0.5 points)\n",
    "\n",
    "Create a feature matrix by merging the non-lexical features with the lexical feature set\n",
    "\n",
    "- `features_set_ngrams`, obtained by merging `other_features_matrix` and `ngrams_matrix_tfidf`\n",
    "\n",
    "\n",
    "*Hint*: the numpy.concatenate() method can be used to concatenate two matrices\n",
    "\n",
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splitting\n",
    "\n",
    "**5. Development and Test Set** (0.5 points)\n",
    "\n",
    "As a first step, split the whole dataset into a development and a test set: randomly select 10% of the sentences to be used in the next phase of this activity (i.e. \"Test the best performing model\"). This is done mainly to prevent our models from over-fitting the training data.\n",
    "\n",
    "The remaining 90% of our sentences will compose the development set on which we are going to train our model.\n",
    "\n",
    "Divide the ids of our matrix into a test set (approximately 10% of the ids of the rows of our matrices) and a development set (the remaining rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dev_test_sets():\n",
    "    assert len(test_set_row_ids) == 2039\n",
    "    assert len(development_set_row_ids) == 18358\n",
    "    assert sorted(list(test_set_row_ids) + list(development_set_row_ids)) == [i for i in range(20397)]\n",
    "    assert sorted(list(test_set_row_ids)) + sorted(list(development_set_row_ids)) != [i for i in range(20397)]\n",
    "    assert sorted(list(development_set_row_ids)) + sorted(list(test_set_row_ids)) != [i for i in range(20397)]\n",
    "    \n",
    "test_dev_test_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, we will see how a Machine Learning algorithm we discussed in class, Logistic Regression, performs with the dataset we've created in the previous phase. For implementing Logistic Regression, we'll use the `linear_model` module available in the `scikit-learn` library (**please note that you are not allowed to use any other module of this library**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing our ML algorithm\n",
    "\n",
    "The following example shows how you can use the `scikit-learn` library to implement Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some dummy datasets\n",
    "\n",
    "X_train = np.random.randint(3, size=(20, 6))  # dummy datatset of training data\n",
    "X_test = np.random.randint(3, size=(5, 6))  # dummy datatset of test data\n",
    "\n",
    "y_train = np.random.randint(3, size=20)  # dummy labels for the training data\n",
    "y_test = np.random.randint(3, size=5)  # dummy labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a minimal Logistic Regression\n",
    "\n",
    "# Create logistic regression object\n",
    "regr = linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "#Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained a model, you would then estimate its performance by comparing `y_pred` (the predicted labels) and `y_test` (the gold standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures\n",
    "\n",
    "To evaluate the quality of a model, we generate predictions on a held-out set (for which we have the correct annotations) and we calculate:\n",
    "\n",
    "**Accuracy**: the percentage of inputs documents (i.e., sentences) correctly classified\n",
    "\n",
    "**Precision, Recall and F-measure** are calculated from the following estimates:\n",
    "\n",
    "- True Positives: fraction of sentences that we correctly identified as relevant\n",
    "- True Negatives: fraction of sentences that we correctly identified as irrelevant\n",
    "- False Positives (or Type I errors): irrelevant sentences that we incorrectly identified as relevant\n",
    "- False Negatives (or Type II errors): relevant sentences that we incorrectly identified as irrelevant\n",
    "\n",
    "<img src=\"https://www.nltk.org/images/precision-recall.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "- **Precision**: how many of the items that we identified as relevant were actually relevant: $\\frac{\\text{TP}}{(\\text{TP}+\\text{FP})}$.\n",
    "\n",
    "\n",
    "- **Recall**: how many of the relevant items we identified we identified as relevant, is $\\frac{\\text{TP}}{(\\text{TP}+\\text{FN})}$,\n",
    "\n",
    "\n",
    "- **F$_1$-measure**: the harmonic mean of the precision and recall: $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Below we will train our logistic regression model with two feature sets:\n",
    "\n",
    "- `other_features_matrix` alone\n",
    "\n",
    "- `features_set_ngrams`, which includes `other_features_matrix` as well as our $n$-grams features\n",
    "\n",
    "When comparing different models, it is better to perform multiple evaluations on different portions of our develpment set, and then combining the scores from those evaluations. This techinique is called **cross validation**.\n",
    "\n",
    "Now, we will divide our development set into **5 subsets** called folds. For each fold, we train our models using all the data in the develpment set except those in this fold, and then we test our models on this fold. We then combine our evaluation to compute the overal score of our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NumPy\n",
    "\n",
    "Some of the computations you will perform below, such as creating a training set from the development set, will require selecting some elements from a NumPy array or matrix. We actively encourage you to perform such operations using matrices entirely, i.e., using NumPy methods such as `numpy.take`, rather than to create `for` loops. Practice with NumPy will be essential in your follow-up course *Machine Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Logistic regression training and cross validation** (3 points). Follows these steps to compare our settings:\n",
    "\n",
    "1. Shuffle development dataset (you may consider using the `random.shuffle()` method)\n",
    "\n",
    "2. Split the shuffled development set into 5 groups (you may consider using the `np.array_split()` method)\n",
    "\n",
    "3. For each fold:\n",
    "    - take the fold as the held-out data set and the remaining folds together as a training set\n",
    "    - train Logistic Regression with each feature setting (lemmas vs. ngrams) on the training set\n",
    "    - evaluate each ML algorithm + feature setting on the held-out set\n",
    "    - for each model (manually, i.e., without using existing libraries) calculate accuracy, precision, recall and f-measure and store these results\n",
    "    \n",
    "4. for each feature setting, average the performance measures over all the folds in cross-validation (this is called **macro-averaging**)\n",
    "\n",
    "5. choose the best performing feature setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation on a test set\n",
    "\n",
    "**7. Test the best performing model** (1 point)\n",
    "\n",
    "Assess the performance of the better model identified in the previous step. For this evaluation, we will use as training set the full development set that we've used fro the 5-folds cross-validation, corresponding to 90% of our sentences). We will then evaluate out model on the remaining 10% of sentences that we've never used (up to now), i.e. the `test set`.\n",
    "\n",
    "We will compare the prediction of our model against the human annotations and we will **manually** calculate:\n",
    "\n",
    "- its accuracy\n",
    "\n",
    "- its precision, recall and f-measure\n",
    "\n",
    "- the confusion matrix (fell free to use the `matplotlib.pyplot.imshow()` method to plot it, but bear in mind that the confusion matrix should be calculated manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Now instead of logistic regression we will experiment with neural networks for this classification task. The goal of this exercise is not to find a classifier with the best out-of-sample performance, but rather to see the practical impact of various choices in setting up a neural network (some of which you have seen in the lecture, some not).\n",
    "\n",
    "We will use Keras, which is a higher-level interface that can be used on top of tensorflow. It makes many common deep learning tasks very easy to perform. The following code sets up a simple neural network with one hidden layer of 20 neurons. We will use the Stochastic Gradient Descent (SGD) optimizer; see documentation here: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation='tanh'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "sgd = tf.keras.optimizers.SGD()\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer uses `tanh` as its nonlinear activation function. The output layer uses `softmax`, which is also a smooth nonlinear function, but has the special property that it transforms an arbitrary real-valued vector into a vector of numbers between 0 and 1, summing to 1. These values are interpreted as likelihoods for the classes, just as for logistic regression. The network is trained to minimize the loss function `sparse_categorical_crossentropy`, which corresponds to maximizing that likelihood, again like logistic regression.\n",
    "\n",
    "**Note:** A loss function `categorical_crossentropy` also exists in Keras. If the labels were encoded as \"one-hot\" vectors instead of as integers, `categorical_crossentropy` would be the right loss function to use instead of the `sparse_` one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Train a simple neural network** Using `features_set_ngrams`, fit this model to your development data (with `epochs=30`; in each \"epoch\", all data points are passed through the network forward and backward once). Then evaluate the model on the test data (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Neural network parameter exploration** (2.5 points)\n",
    "\n",
    "The `model.fit` function returns a History object, which stores among other things the training (in-sample) losses per epoch. If the History object is called `hist[i]`, the losses are `hist[i].history['loss']`.\n",
    "\n",
    "Use this to plot the training losses per epoch for three different values of the `learning_rate` (the step size $\\eta$, or \"eta\", used by stochastic gradient descent): 0.1, 0.01 (the default), and 0.001. (To write your solution, start by copy-pasting the code from above and putting a for-loop around it.)\n",
    "What do you observe?\n",
    "\n",
    "Similarly experiment with each of the following aspects of the neural network:\n",
    "* You can pass different values of `batch_size` to `model.fit`. For `batch_size=1`, we get standard Stochastic Gradient Descent (SGD) where the gradient is computed for one data point at a time. For a batch size at least as large as the training data, the gradient is computed for all these data points together (this is batch GD). The default value of 32 is in between these two extremes (this is called a \"minibatch\");\n",
    "* Gradient descent only looks at the gradient, and only at the point of the current weights. More advanced optimizers take more information into account, such as second derivatives and recently visited points. A popular choice is `optimizer='adam'` instead of `optimizer=sgd`;\n",
    "* The number of epochs used when fitting the model (if the 30 epochs we chose before led to worse performance than logistic regression, try increasing the number of epochs; see if the accuracy seems to be converging);\n",
    "* The activation function in the hidden layer (e.g. `'relu'` instead of `'tanh'`);\n",
    "* The number of neurons in the hidden layer.\n",
    "\n",
    "For each of these, write code that tries out models with different values of that setting, while keeping all other settings constant at values you choose based on the previous experiments. Your code must produce relevant output when the notebook is run. This can be one of many different things, e.g. training losses per epoch, test accuracy, computation time, ..., so for each setting, pick one that you expect to be interesting, and write code to measure it. In the corresponding text cell, write down your conclusions from your code's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate: your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size: your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer: your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs: your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function: your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of neurons: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of neurons: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Questions\n",
    "\n",
    "**10. What was your better logistic regression model? Intuitively, why do you think that it performed better?** (1 point)\n",
    "\n",
    "**Answer.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Did the neural networks perform better or worse than logistic regression? How can you interpret that result?** (0.5 points)\n",
    "\n",
    "**Answer.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
